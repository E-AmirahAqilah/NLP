{"cells":[{"cell_type":"markdown","metadata":{"id":"XXIADJR1yrpp"},"source":["<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left>  <img src=\"../images/NLP.jpeg\" style=\"height: 200px;\" align=right, width=\"300\">\n","# Natural Language Processing"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"5uPPezQJyrpv"},"source":["# Contents:\n","\n","- Introduction to Natural Language Processing\n","- Sentiment Analysis\n","    - Model Selection in scikit-learn\n","    - Extracting features\n","        - Bag-of-words\n","        - Exercise A\n","    - Logistic Regression classification\n","    - Tfidf\n","        - Exercise B\n","    - N-gram\n","- Text Classification\n","    - Using sklearn's NaiveBayes Classifier\n","        - Exercise C"]},{"cell_type":"markdown","metadata":{"id":"Aru-U-59yrpx"},"source":["# 1. Introduction to Natural Language Processing\n","NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as â€“ automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.\n","\n","What better way than to use a popular use case application: Amazon review sentiment analysis, to better understand how text information can be parsed and processed into something useful for ML.\n"]},{"cell_type":"markdown","metadata":{"id":"v5MVJFZKyrp1"},"source":["# 2. Case Study: Sentiment Analysis\n","\n","We will be working on a large dataset of reviews of unlocked mobile phones sold on Amazon.com that has been collected by Crawlers et al. in December, 2016. The Amazon reviews dataset consists of 400 thousand reviews to find out insights with respect to reviews, ratings, price and their relationships.\n","\n","#### Dataset Content\n","\n","Given below are the fields:\n","\n","- Product Title\n","- Brand\n","- Price\n","- Rating\n","- Review text\n","- Number of people who found the review helpful\n","\n","Our main end goal here is to learn how to extract meaningful information from a subset of these reviews to build a machine learning model that can predict whether a certain reviewer liked or disliked a mobile phones."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"NSsIeBak9Meg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3_HY22wjyrp2"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":530},"id":"URXE_kU8yrp4","executionInfo":{"status":"error","timestamp":1729559869733,"user_tz":-480,"elapsed":3199,"user":{"displayName":"Andrew Yau","userId":"10674489837470466957"}},"outputId":"0576e4eb-d36b-48b4-921b-bfe51a3fa4cb"},"outputs":[{"output_type":"error","ename":"ParserError","evalue":"Error tokenizing data. C error: EOF inside string starting at row 377807","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-b66231eca968>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/data/Amazon_Unlocked_Mobile.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# shuffle rows of dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 377807"]}],"source":["# Read in the data\n","df = pd.read_csv(\"/content/data/Amazon_Unlocked_Mobile.csv\", encoding=\"utf8\")\n","\n","# shuffle rows of dataframe\n","df = df.sample(frac=0.1, random_state=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"scrolled":true,"id":"y_TMdXSQyrp4"},"outputs":[],"source":["# Drop missing values\n","df.dropna(inplace=True)\n","\n","# Remove any 'neutral' ratings equal to 3\n","df = df[df['Rating'] != 3]\n","\n","# Encode 4s and 5s as 1 (rated positively)\n","# Encode 1s and 2s as 0 (rated poorly)\n","df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)"]},{"cell_type":"markdown","metadata":{"id":"mdDbuZ1Syrp5"},"source":["# Model Selection in scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"UOzj9ltCyrp6"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split data into train and test subsets\n","X_train, X_test, y_train, y_test = train_test_split(,\n","                                                    ?,\n","                                                    random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWDa9rolyrp6"},"outputs":[],"source":["# What is the review number 10 in the X_train set\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVtqzw2Jyrp7"},"outputs":[],"source":["# X_train size\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAFJA0Asyrp7"},"outputs":[],"source":["# X_test size"]},{"cell_type":"markdown","metadata":{"id":"YIbrIiZ7yrp7"},"source":["# Extracting features from text files\n","\n","\n","Text files are actually series of words (ordered). In order to run machine learning algorithms we need to convert the text files into numerical feature vectors. We will be using bag of words model.\n","\n","## Bag-of-words (BOW)\n","BOW model allows us to represent text as numerical feature vectors. The idea behind BOW is quite simple and can be summarized as follows:\n","- 1) Create a vocabulary of unique tokens (or words) from the entire set\n","    of documents.\n","- 2) Construct a feature vector from each document that contains the counts of how often each word occurs in the particular document.\n","\n","Since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will consist of mostly zeros, which is why we call them sparse. For this reason we say that bags of words are typically <b>high-dimensional sparse datasets</b>.\n","\n","{for our example. Briefly, we segment each text file into words (for English splitting by space), and count # of times each word occurs in each document and finally assign each word an integer id. Each unique word in our dictionary will correspond to a feature (descriptive feature).}\n","\n","\n","### Transform words into vectors (CountVectorizer)\n","To construct a bag-of-words model based on the word counts in the respective documents, we can use the `CountVectorizer` class implemented in `scikit-learn`. As we will see in the following codes, the `CountVectorizer` class takes an array of text data, which can be documents or just sentences, and constructs the bag-of-words model for us:\n","\n","Scikit-learn has a high level component which will create feature vectors for us <b>â€˜CountVectorizerâ€™</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"KKdcX-0Qyrp8"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","docs = np.array([\n","    'The sun is shining',\n","    'The weather is sweet',\n","    'The sun is shining and the weather is sweet'])\n","\n","# Fit the CountVectorizer to the training data\n","vect1=CountVectorizer().fit(docs)\n","\n","# transform the documents in the training data to a document-term matrix.\n","bag = vect1.transform(docs)"]},{"cell_type":"markdown","metadata":{"id":"P4SKKddjyrp8"},"source":["## <font color=green> Exercise A</font>\n","\n","1) Do CountVectorizer for training data\n","\n","2) Detedrmine:\n","- The number of features\n","- The shape of sparse matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BaTd0TPCyrp9"},"outputs":[],"source":["# Your code here\n","vect"]},{"cell_type":"markdown","metadata":{"id":"91BzTWW_yrp9"},"source":["# Logistic Regression classification\n","\n","We will train a logistic regression model to classify the  Amazon reviews into positive and negative reviews by using feature matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"MsearwHyyrp-"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","# Train the model\n","model = LogisticRegression()\n","model.fit(X_train_vectorized, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aO8FU3EEyrp-"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","# Predict the transformed test documents\n","predictions = model.predict(vect.transform(X_test))\n","y_proba = model.predict_proba(vect.transform(X_test))\n","\n","print('AUC: ', roc_auc_score(y_test, y_proba[:,1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhION924yrp_"},"outputs":[],"source":["model.coef_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAV7hiapyrp_"},"outputs":[],"source":["model.coef_[0].argsort()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZ1y7zMvyrqA"},"outputs":[],"source":["# get the feature names as numpy array\n","feature_names = np.array(vect.get_feature_names())\n","\n","# Sort the coefficients from the model\n","sorted_coef_index = model.coef_[0].argsort()\n","\n","# Find the 10 smallest and 10 largest coefficients\n","print('Smallest Coefs:' )\n","print(feature_names[sorted_coef_index[:10]])\n","\n","print('\\n Largest Coefs:')\n","print(feature_names[sorted_coef_index[:-11:-1]])"]},{"cell_type":"markdown","metadata":{"id":"tlgR1J9vyrqA"},"source":["# Tfidf\n","\n","When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don't contain useful or discriminatory information. In this subsection, we will learn about a useful technique called **term frequency-inverse document frequency** (*tf-idf*) that can be used to downweight those frequently occurring words in the feature vectors. On the other words by tf-idf we can reduce the weightage of more common words like (the, is, an etc.) which occurs in all document.\n","\n","The *tf-idf* can be defined as the product of the term frequency and the inverse document frequency:\n","\n","\\begin{align}\n","\\textit{tf-idf}(t,d) = tf(t,d) \\times idf(t,d)\n","\\end{align}\n","\n","Here the <font color=green><b> *tf(t,d)* </b></font> is the term frequency that equal to **Count of word / Total words, in each document**. The inverse document frequency *idf(t,d)* can be calculated as:\n","\n","\\begin{align}\n","idf(t,d) = log\\frac{n_d}{\\text{df(d,t)}}\n","\\end{align}\n","\n","where <font color=green><b> $n_d$ </b></font> is **the total number of documents**, and <font color=green><b>*df(d,t)*</b></font> is **the number of documents *d* that contain the term t**. Note that the log is used to ensure that low document frequencies are not given too much weight.\n","\n","\n","scikit-learn implements yet another vectorizer, the TfidfVectorizer, that creates feature vectors as tf-idfs.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5b1mBRLhyrqB"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","docs = np.array([\n","    'The sun is shining',\n","    'The weather is sweet',\n","    'The sun is shining and the weather is sweet'])\n","\n","vect2 = TfidfVectorizer().fit(docs)\n","bag2 = vect2.transform(docs)\n","bag2.toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tye0L6xgyrqC"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Fit the TfidfVectorizer to the training data\n","vect = TfidfVectorizer(min_df=5).fit(X_train)\n","X_train_vectorized = vect.transform(X_train)\n","\n","model = LogisticRegression()\n","model.fit(X_train_vectorized, y_train)\n","\n","predictions = model.predict(vect.transform(X_test))\n","y_proba = model.predict_proba(vect.transform(X_test))\n","\n","print('AUC: ', roc_auc_score(y_test, y_proba[:,1]))"]},{"cell_type":"markdown","metadata":{"id":"9-IHZEgByrqC"},"source":["## <font color=green> Exercise B</font>\n","- Predict two below reviews as negetive or positive using our model:\n","\n","      ['no an issue, phone is working', 'an issue, phone is not working']      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IW_kg_ELyrqC"},"outputs":[],"source":["# Your code here\n","x_test = ['no an issue, phone is working', 'an issue, phone is not working']\n","vect.transform(x_test)"]},{"cell_type":"markdown","metadata":{"id":"mTq6V9TAyrqC"},"source":["# n-grams\n","\n","The sequence of items in the bag-of-words model that we just created is also called the 1-gram or unigram model â€” each item or token in the vocabulary represents a single word. Generally, <b>the contiguous sequences of items in NLP</b> â€” words, letters, or symbolsâ€” is also called an n-gram. The choice of the number n in the n-gram model depends on the particular application. For instance, spam filtering applications tend to use n=3 or n=4 for good performances.\n","To summarize the concept of the n-gram representation, the 1-gram and 2-gram representations of our first document \"the sun is shining\" would be constructed as follows:\n","- 1-gram: \"the\", \"sun\", \"is\", \"shining\"\n","- 2-gram: \"the sun\", \"sun is\", \"is shining\"\n","\n","The CountVectorizer class in scikit-learn allows us to use different n-gram models via its ngram_range parameter. By default, it uses a 1-gram representation."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"5J0Zo8JlyrqE"},"outputs":[],"source":["# Try 2-gram representation\n","docs = np.array([\n","    'The sun is shining',\n","    'The weather is sweet',\n","    'The sun is shining and the weather is sweet'])\n","\n","vect3=CountVectorizer(ngram_range=(1,2)).fit(docs)\n","bag3=vect3.transform(docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"_EGAdaP3yrqE"},"outputs":[],"source":["vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n","\n","X_train_vectorized = vect.transform(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjTzcY8fyrqF"},"outputs":[],"source":["model = LogisticRegression()\n","model.fit(X_train_vectorized, y_train)\n","\n","predictions = model.predict(vect.transform(X_test))\n","y_proba = model.predict_proba(vect.transform(X_test))\n","\n","print('AUC: ', roc_auc_score(y_test, y_proba[:,1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-9EtdDOyrqL"},"outputs":[],"source":["feature_names = np.array(vect.get_feature_names())\n","\n","sorted_coef_index = model.coef_[0].argsort()\n","\n","print('Smallest Coefs:' )\n","print(feature_names[sorted_coef_index[:10]])\n","\n","print('\\n Largest Coefs:')\n","print(feature_names[sorted_coef_index[:-11:-1]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSvtxxnIyrqL"},"outputs":[],"source":["print(model.predict(vect.transform(['no an issue, phone is working',\n","                                    'an issue, phone is not working'])))"]},{"cell_type":"markdown","metadata":{"id":"wGhM8hblyrqL"},"source":["# Text Classification\n","\n","## Using sklearn's NaiveBayes Classifier\n","\n","\n","### <font color=green> Exercise C</font>\n","1. Do text classification for the Amazon reviews dataset using NaiveBayes Classifier\n","2. Evaluate your model classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Sk-wN8CbyrqM"},"outputs":[],"source":["from sklearn import naive_bayes\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"TyBOE9yCyrqM"},"outputs":[],"source":["# Your code here"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
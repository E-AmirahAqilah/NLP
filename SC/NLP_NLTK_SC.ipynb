{"cells":[{"cell_type":"markdown","metadata":{"id":"CQSw0ksipxj8"},"source":["<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left>  <img src=\"../images/NLP.jpeg\" style=\"height: 200px;\" align=right, width=\"300\">\n","# Natural Language Processing"]},{"cell_type":"markdown","metadata":{"id":"I-2bh_6opxj-"},"source":["# Contents:\n","\n","- Natural Language Toolkit (NLTK)\n","- Install NLTK\n","- Text Preprocessing\n","    - Tokenization\n","    - Noise Removal\n","        - Language Stop Words\n","        - Filter Out Punctuation\n","        - URLs or links\n","            - Regular Expression\n","            - The most common uses of regular expressions\n","            - Exercise A\n","            - Two useful methods\n","            - Exercise B\n","        - Social media entities\n","            - Exercise C\n","    - Lexicon Normalization\n","        - Stemming\n","        - Lemmatization\n","- Part of Speech Tagging\n","- Named entity recognition\n","- Some useful functions\n","    - Finding specific words\n","    - Finding unique words\n","    - Frequency of words\n","    - Exercise E"]},{"cell_type":"markdown","metadata":{"id":"pbPfigSCpxkA"},"source":["# Natural Language Toolkit (NLTK) module with Python\n","\n","The NLTK module is a massive tool kit, aimed at helping you with the entire Natural Language Processing (NLP) methodology. It is an open source library in python. NLTK will aid you with everything from splitting sentences from paragraphs, splitting up words, recognizing the part of speech of those words, highlighting the main subjects, and then even with helping your machine to understand what the text is all about\n","\n","#### Advantages of NLTK\n","- Has support for most NLP tasks\n","- Provide acsses to numerous text corpora\n","\n","\n","Let to explain some terms:\n","- Tokens – words or sentences or entities present in the text\n","- Tokenization – process of converting a text into tokens\n","- Lexicon – Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons.\n","- Corpus – Body of text, singular. Corpora is the plural of this. Example: A collection of medical journals."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"FDeWqlobpxkB"},"source":["# Install NLTK"]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"SWx87YukpxkC","executionInfo":{"status":"ok","timestamp":1731906211463,"user_tz":-480,"elapsed":8994,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"5318ba09-0cdf-438a-eb8c-412d22ae07b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"tKuHL59UpxkD","executionInfo":{"status":"ok","timestamp":1731906444819,"user_tz":-480,"elapsed":231631,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"5d19b801-071e-4fe7-ec44-684f8f992ad3"},"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> d\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> l\n","Packages:\n","  [ ] abc................. Australian Broadcasting Commission 2006\n","  [ ] alpino.............. Alpino Dutch Treebank\n","  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n","  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n","  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n","  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n","  [ ] basque_grammars..... Grammars for Basque\n","  [ ] bcp47............... BCP-47 Language Tags\n","  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n","                           Extraction Systems in Biology)\n","  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n","  [ ] book_grammars....... Grammars from NLTK Book\n","  [ ] brown............... Brown Corpus\n","  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n","  [ ] cess_cat............ CESS-CAT Treebank\n","  [ ] cess_esp............ CESS-ESP Treebank\n","  [ ] chat80.............. Chat-80 Data Files\n","  [ ] city_database....... City Database\n","  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n","  [ ] comparative_sentences Comparative Sentence Dataset\n","Hit Enter to continue: \n","  [ ] comtrans............ ComTrans Corpus Sample\n","  [ ] conll2000........... CONLL 2000 Chunking Corpus\n","  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n","  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n","                           and Basque Subset)\n","  [ ] crubadan............ Crubadan Corpus\n","  [ ] dependency_treebank. Dependency Parsed Treebank\n","  [ ] dolch............... Dolch Word List\n","  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n","                           Corpus\n","  [ ] extended_omw........ Extended Open Multilingual WordNet\n","  [ ] floresta............ Portuguese Treebank\n","  [ ] framenet_v15........ FrameNet 1.5\n","  [ ] framenet_v17........ FrameNet 1.7\n","  [ ] gazetteers.......... Gazeteer Lists\n","  [ ] genesis............. Genesis Corpus\n","  [ ] gutenberg........... Project Gutenberg Selections\n","  [ ] ieer................ NIST IE-ER DATA SAMPLE\n","  [ ] inaugural........... C-Span Inaugural Address Corpus\n","  [ ] indian.............. Indian Language POS-Tagged Corpus\n","  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n","                           ChaSen format)\n","Hit Enter to continue: \n","  [ ] kimmo............... PC-KIMMO Data Files\n","  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n","  [ ] large_grammars...... Large context-free and feature-based grammars\n","                           for parser comparison\n","  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n","  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n","                           part-of-speech tags\n","  [ ] machado............. Machado de Assis -- Obra Completa\n","  [ ] masc_tagged......... MASC Tagged Corpus\n","  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n","  [ ] maxent_ne_chunker_tab ACE Named Entity Chunker (Maximum entropy)\n","  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n","  [ ] maxent_treebank_pos_tagger_tab Treebank Part of Speech Tagger (Maximum entropy)\n","  [ ] moses_sample........ Moses Sample Models\n","  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n","  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n","  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n","                           2015) subset of the Paraphrase Database.\n","  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n","  [ ] nombank.1.0......... NomBank Corpus 1.0\n","  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n","Hit Enter to continue: abc\n","  [ ] nps_chat............ NPS Chat\n","  [ ] omw-1.4............. Open Multilingual Wordnet\n","  [ ] omw................. Open Multilingual Wordnet\n","  [ ] opinion_lexicon..... Opinion Lexicon\n","  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n","  [ ] paradigms........... Paradigm Corpus\n","  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n","                           Evaluation Shared Task\n","  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n","                           character properties in Perl\n","  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n","  [ ] pl196x.............. Polish language of the XX century sixties\n","  [ ] porter_test......... Porter Stemmer Test Files\n","  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n","  [ ] problem_reports..... Problem Report Corpus\n","  [ ] product_reviews_1... Product Reviews (5 Products)\n","  [ ] product_reviews_2... Product Reviews (9 Products)\n","  [ ] propbank............ Proposition Bank Corpus 1.0\n","  [ ] pros_cons........... Pros and Cons\n","  [ ] ptb................. Penn Treebank\n","  [ ] punkt............... Punkt Tokenizer Models\n","Hit Enter to continue: \n","  [ ] punkt_tab........... Punkt Tokenizer Models\n","  [ ] qc.................. Experimental Data for Question Classification\n","  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n","                           version\n","  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n","                           Portuguesa)\n","  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n","  [ ] sample_grammars..... Sample Grammars\n","  [ ] semcor.............. SemCor 3.0\n","  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n","  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n","  [ ] sentiwordnet........ SentiWordNet\n","  [ ] shakespeare......... Shakespeare XML Corpus Sample\n","  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n","  [ ] smultron............ SMULTRON Corpus Sample\n","  [ ] snowball_data....... Snowball Data\n","  [ ] spanish_grammars.... Grammars for Spanish\n","  [ ] state_union......... C-Span State of the Union Address Corpus\n","  [ ] stopwords........... Stopwords Corpus\n","  [ ] subjectivity........ Subjectivity Dataset v1.0\n","  [ ] swadesh............. Swadesh Wordlists\n","Hit Enter to continue: \n","  [ ] switchboard......... Switchboard Corpus Sample\n","  [ ] tagsets............. Help on Tagsets\n","  [ ] tagsets_json........ Help on Tagsets (JSON)\n","  [ ] timit............... TIMIT Corpus Sample\n","  [ ] toolbox............. Toolbox Sample Files\n","  [ ] treebank............ Penn Treebank Sample\n","  [ ] twitter_samples..... Twitter Samples\n","  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n","                           (Unicode Version)\n","  [ ] udhr................ Universal Declaration of Human Rights Corpus\n","  [ ] unicode_samples..... Unicode Samples\n","  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n","  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n","  [ ] vader_lexicon....... VADER Sentiment Lexicon\n","  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n","  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n","  [ ] webtext............. Web Text Corpus\n","  [ ] wmt15_eval.......... Evaluation data from WMT15\n","  [ ] word2vec_sample..... Word2Vec Sample\n","  [ ] wordnet2021......... Open English Wordnet 2021\n","  [ ] wordnet2022......... Open English Wordnet 2022\n","Hit Enter to continue: \n","  [ ] wordnet31........... Wordnet 3.1\n","  [ ] wordnet............. WordNet\n","  [ ] wordnet_ic.......... WordNet-InfoContent\n","  [ ] words............... Word Lists\n","  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n","                           English Prose\n","\n","Collections:\n","  [ ] all-corpora......... All the corpora\n","  [ ] all-nltk............ All packages available on nltk_data gh-pages\n","                           branch\n","  [ ] all................. All packages\n","  [ ] book................ Everything used in the NLTK Book\n","  [ ] popular............. Popular packages\n","  [ ] tests............... Packages for running tests\n","  [ ] third-party......... Third-party data packages\n","\n","([*] marks installed packages)\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> all\n"]},{"output_type":"stream","name":"stderr","text":["    Downloading collection 'all'\n","       | \n","       | Downloading package abc to /root/nltk_data...\n","       |   Unzipping corpora/abc.zip.\n","       | Downloading package alpino to /root/nltk_data...\n","       |   Unzipping corpora/alpino.zip.\n","       | Downloading package averaged_perceptron_tagger to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","       | Downloading package averaged_perceptron_tagger_eng to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","       | Downloading package averaged_perceptron_tagger_ru to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n","       | Downloading package averaged_perceptron_tagger_rus to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/averaged_perceptron_tagger_rus.zip.\n","       | Downloading package basque_grammars to /root/nltk_data...\n","       |   Unzipping grammars/basque_grammars.zip.\n","       | Downloading package bcp47 to /root/nltk_data...\n","       | Downloading package biocreative_ppi to /root/nltk_data...\n","       |   Unzipping corpora/biocreative_ppi.zip.\n","       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n","       |   Unzipping models/bllip_wsj_no_aux.zip.\n","       | Downloading package book_grammars to /root/nltk_data...\n","       |   Unzipping grammars/book_grammars.zip.\n","       | Downloading package brown to /root/nltk_data...\n","       |   Unzipping corpora/brown.zip.\n","       | Downloading package brown_tei to /root/nltk_data...\n","       |   Unzipping corpora/brown_tei.zip.\n","       | Downloading package cess_cat to /root/nltk_data...\n","       |   Unzipping corpora/cess_cat.zip.\n","       | Downloading package cess_esp to /root/nltk_data...\n","       |   Unzipping corpora/cess_esp.zip.\n","       | Downloading package chat80 to /root/nltk_data...\n","       |   Unzipping corpora/chat80.zip.\n","       | Downloading package city_database to /root/nltk_data...\n","       |   Unzipping corpora/city_database.zip.\n","       | Downloading package cmudict to /root/nltk_data...\n","       |   Unzipping corpora/cmudict.zip.\n","       | Downloading package comparative_sentences to\n","       |     /root/nltk_data...\n","       |   Unzipping corpora/comparative_sentences.zip.\n","       | Downloading package comtrans to /root/nltk_data...\n","       | Downloading package conll2000 to /root/nltk_data...\n","       |   Unzipping corpora/conll2000.zip.\n","       | Downloading package conll2002 to /root/nltk_data...\n","       |   Unzipping corpora/conll2002.zip.\n","       | Downloading package conll2007 to /root/nltk_data...\n","       | Downloading package crubadan to /root/nltk_data...\n","       |   Unzipping corpora/crubadan.zip.\n","       | Downloading package dependency_treebank to /root/nltk_data...\n","       |   Unzipping corpora/dependency_treebank.zip.\n","       | Downloading package dolch to /root/nltk_data...\n","       |   Unzipping corpora/dolch.zip.\n","       | Downloading package europarl_raw to /root/nltk_data...\n","       |   Unzipping corpora/europarl_raw.zip.\n","       | Downloading package extended_omw to /root/nltk_data...\n","       | Downloading package floresta to /root/nltk_data...\n","       |   Unzipping corpora/floresta.zip.\n","       | Downloading package framenet_v15 to /root/nltk_data...\n","       |   Unzipping corpora/framenet_v15.zip.\n","       | Downloading package framenet_v17 to /root/nltk_data...\n","       |   Unzipping corpora/framenet_v17.zip.\n","       | Downloading package gazetteers to /root/nltk_data...\n","       |   Unzipping corpora/gazetteers.zip.\n","       | Downloading package genesis to /root/nltk_data...\n","       |   Unzipping corpora/genesis.zip.\n","       | Downloading package gutenberg to /root/nltk_data...\n","       |   Unzipping corpora/gutenberg.zip.\n","       | Downloading package ieer to /root/nltk_data...\n","       |   Unzipping corpora/ieer.zip.\n","       | Downloading package inaugural to /root/nltk_data...\n","       |   Unzipping corpora/inaugural.zip.\n","       | Downloading package indian to /root/nltk_data...\n","       |   Unzipping corpora/indian.zip.\n","       | Downloading package jeita to /root/nltk_data...\n","       | Downloading package kimmo to /root/nltk_data...\n","       |   Unzipping corpora/kimmo.zip.\n","       | Downloading package knbc to /root/nltk_data...\n","       | Downloading package large_grammars to /root/nltk_data...\n","       |   Unzipping grammars/large_grammars.zip.\n","       | Downloading package lin_thesaurus to /root/nltk_data...\n","       |   Unzipping corpora/lin_thesaurus.zip.\n","       | Downloading package mac_morpho to /root/nltk_data...\n","       |   Unzipping corpora/mac_morpho.zip.\n","       | Downloading package machado to /root/nltk_data...\n","       | Downloading package masc_tagged to /root/nltk_data...\n","       | Downloading package maxent_ne_chunker to /root/nltk_data...\n","       |   Unzipping chunkers/maxent_ne_chunker.zip.\n","       | Downloading package maxent_ne_chunker_tab to\n","       |     /root/nltk_data...\n","       |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n","       | Downloading package maxent_treebank_pos_tagger to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","       | Downloading package maxent_treebank_pos_tagger_tab to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/maxent_treebank_pos_tagger_tab.zip.\n","       | Downloading package moses_sample to /root/nltk_data...\n","       |   Unzipping models/moses_sample.zip.\n","       | Downloading package movie_reviews to /root/nltk_data...\n","       |   Unzipping corpora/movie_reviews.zip.\n","       | Downloading package mte_teip5 to /root/nltk_data...\n","       |   Unzipping corpora/mte_teip5.zip.\n","       | Downloading package mwa_ppdb to /root/nltk_data...\n","       |   Unzipping misc/mwa_ppdb.zip.\n","       | Downloading package names to /root/nltk_data...\n","       |   Unzipping corpora/names.zip.\n","       | Downloading package nombank.1.0 to /root/nltk_data...\n","       | Downloading package nonbreaking_prefixes to\n","       |     /root/nltk_data...\n","       |   Unzipping corpora/nonbreaking_prefixes.zip.\n","       | Downloading package nps_chat to /root/nltk_data...\n","       |   Unzipping corpora/nps_chat.zip.\n","       | Downloading package omw to /root/nltk_data...\n","       | Downloading package omw-1.4 to /root/nltk_data...\n","       | Downloading package opinion_lexicon to /root/nltk_data...\n","       |   Unzipping corpora/opinion_lexicon.zip.\n","       | Downloading package panlex_swadesh to /root/nltk_data...\n","       | Downloading package paradigms to /root/nltk_data...\n","       |   Unzipping corpora/paradigms.zip.\n","       | Downloading package pe08 to /root/nltk_data...\n","       |   Unzipping corpora/pe08.zip.\n","       | Downloading package perluniprops to /root/nltk_data...\n","       |   Unzipping misc/perluniprops.zip.\n","       | Downloading package pil to /root/nltk_data...\n","       |   Unzipping corpora/pil.zip.\n","       | Downloading package pl196x to /root/nltk_data...\n","       |   Unzipping corpora/pl196x.zip.\n","       | Downloading package porter_test to /root/nltk_data...\n","       |   Unzipping stemmers/porter_test.zip.\n","       | Downloading package ppattach to /root/nltk_data...\n","       |   Unzipping corpora/ppattach.zip.\n","       | Downloading package problem_reports to /root/nltk_data...\n","       |   Unzipping corpora/problem_reports.zip.\n","       | Downloading package product_reviews_1 to /root/nltk_data...\n","       |   Unzipping corpora/product_reviews_1.zip.\n","       | Downloading package product_reviews_2 to /root/nltk_data...\n","       |   Unzipping corpora/product_reviews_2.zip.\n","       | Downloading package propbank to /root/nltk_data...\n","       | Downloading package pros_cons to /root/nltk_data...\n","       |   Unzipping corpora/pros_cons.zip.\n","       | Downloading package ptb to /root/nltk_data...\n","       |   Unzipping corpora/ptb.zip.\n","       | Downloading package punkt to /root/nltk_data...\n","       |   Unzipping tokenizers/punkt.zip.\n","       | Downloading package punkt_tab to /root/nltk_data...\n","       |   Unzipping tokenizers/punkt_tab.zip.\n","       | Downloading package qc to /root/nltk_data...\n","       |   Unzipping corpora/qc.zip.\n","       | Downloading package reuters to /root/nltk_data...\n","       | Downloading package rslp to /root/nltk_data...\n","       |   Unzipping stemmers/rslp.zip.\n","       | Downloading package rte to /root/nltk_data...\n","       |   Unzipping corpora/rte.zip.\n","       | Downloading package sample_grammars to /root/nltk_data...\n","       |   Unzipping grammars/sample_grammars.zip.\n","       | Downloading package semcor to /root/nltk_data...\n","       | Downloading package senseval to /root/nltk_data...\n","       |   Unzipping corpora/senseval.zip.\n","       | Downloading package sentence_polarity to /root/nltk_data...\n","       |   Unzipping corpora/sentence_polarity.zip.\n","       | Downloading package sentiwordnet to /root/nltk_data...\n","       |   Unzipping corpora/sentiwordnet.zip.\n","       | Downloading package shakespeare to /root/nltk_data...\n","       |   Unzipping corpora/shakespeare.zip.\n","       | Downloading package sinica_treebank to /root/nltk_data...\n","       |   Unzipping corpora/sinica_treebank.zip.\n","       | Downloading package smultron to /root/nltk_data...\n","       |   Unzipping corpora/smultron.zip.\n","       | Downloading package snowball_data to /root/nltk_data...\n","       | Downloading package spanish_grammars to /root/nltk_data...\n","       |   Unzipping grammars/spanish_grammars.zip.\n","       | Downloading package state_union to /root/nltk_data...\n","       |   Unzipping corpora/state_union.zip.\n","       | Downloading package stopwords to /root/nltk_data...\n","       |   Unzipping corpora/stopwords.zip.\n","       | Downloading package subjectivity to /root/nltk_data...\n","       |   Unzipping corpora/subjectivity.zip.\n","       | Downloading package swadesh to /root/nltk_data...\n","       |   Unzipping corpora/swadesh.zip.\n","       | Downloading package switchboard to /root/nltk_data...\n","       |   Unzipping corpora/switchboard.zip.\n","       | Downloading package tagsets to /root/nltk_data...\n","       |   Unzipping help/tagsets.zip.\n","       | Downloading package tagsets_json to /root/nltk_data...\n","       |   Unzipping help/tagsets_json.zip.\n","       | Downloading package timit to /root/nltk_data...\n","       |   Unzipping corpora/timit.zip.\n","       | Downloading package toolbox to /root/nltk_data...\n","       |   Unzipping corpora/toolbox.zip.\n","       | Downloading package treebank to /root/nltk_data...\n","       |   Unzipping corpora/treebank.zip.\n","       | Downloading package twitter_samples to /root/nltk_data...\n","       |   Unzipping corpora/twitter_samples.zip.\n","       | Downloading package udhr to /root/nltk_data...\n","       |   Unzipping corpora/udhr.zip.\n","       | Downloading package udhr2 to /root/nltk_data...\n","       |   Unzipping corpora/udhr2.zip.\n","       | Downloading package unicode_samples to /root/nltk_data...\n","       |   Unzipping corpora/unicode_samples.zip.\n","       | Downloading package universal_tagset to /root/nltk_data...\n","       |   Unzipping taggers/universal_tagset.zip.\n","       | Downloading package universal_treebanks_v20 to\n","       |     /root/nltk_data...\n","       | Downloading package vader_lexicon to /root/nltk_data...\n","       | Downloading package verbnet to /root/nltk_data...\n","       |   Unzipping corpora/verbnet.zip.\n","       | Downloading package verbnet3 to /root/nltk_data...\n","       |   Unzipping corpora/verbnet3.zip.\n","       | Downloading package webtext to /root/nltk_data...\n","       |   Unzipping corpora/webtext.zip.\n","       | Downloading package wmt15_eval to /root/nltk_data...\n","       |   Unzipping models/wmt15_eval.zip.\n","       | Downloading package word2vec_sample to /root/nltk_data...\n","       |   Unzipping models/word2vec_sample.zip.\n","       | Downloading package wordnet to /root/nltk_data...\n","       | Downloading package wordnet2021 to /root/nltk_data...\n","       | Downloading package wordnet2022 to /root/nltk_data...\n","       |   Unzipping corpora/wordnet2022.zip.\n","       | Downloading package wordnet31 to /root/nltk_data...\n","       | Downloading package wordnet_ic to /root/nltk_data...\n","       |   Unzipping corpora/wordnet_ic.zip.\n","       | Downloading package words to /root/nltk_data...\n","       |   Unzipping corpora/words.zip.\n","       | Downloading package ycoe to /root/nltk_data...\n","       |   Unzipping corpora/ycoe.zip.\n","       | \n","     Done downloading collection all\n"]},{"name":"stdout","output_type":"stream","text":["\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["import nltk\n","nltk.download()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"BxQOOmv3pxkE","executionInfo":{"status":"ok","timestamp":1731909119337,"user_tz":-480,"elapsed":314,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"7fd9a8dd-c388-44ec-9736-f0fc107b4451"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/usr/local/lib/python3.10/dist-packages/nltk/__init__.py'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["nltk.__file__"]},{"cell_type":"markdown","metadata":{"id":"pu3V3XrCpxkE"},"source":["# Text Preprocessing\n","\n","Since, text is the most unstructured form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing.\n","\n","It is comprised of two steps:\n","\n","- <b>Noise Removal</b>\n","    - Language stopwords (commonly used words of a language – is, am, the, of, in etc)\n","    - Punctuations\n","    - URLs or links\n","    - Social media entities (mentions, hashtags)\n","<BR> <BR>    \n","- <b>Lexicon Normalization</b>\n","     - Stemming\n","     - Lemmatization"]},{"cell_type":"markdown","metadata":{"id":"jROTh-CdpxkF"},"source":["## Tokenization\n","Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens.\n","\n","let's show an example of how one might actually tokenize something into tokens with the NLTK module."]},{"cell_type":"code","execution_count":5,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"dHjUILFapxkF","executionInfo":{"status":"ok","timestamp":1731910146084,"user_tz":-480,"elapsed":335,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"b058ea47-78c5-4335-e360-d5fd6cdac25b"},"outputs":[{"output_type":"stream","name":"stdout","text":["['We are attacking on their left flank but are losing many men.', 'we cannot see the enemy army.', 'Nothing else to report.', 'We are ready to attack but are waiting for your orders.']\n","['We', 'are', 'attacking', 'on', 'their', 'left', 'flank', 'but', 'are', 'losing', 'many', 'men', '.', 'we', 'can', 'not', 'see', 'the', 'enemy', 'army', '.', 'Nothing', 'else', 'to', 'report', '.', 'We', 'are', 'ready', 'to', 'attack', 'but', 'are', 'waiting', 'for', 'your', 'orders', '.']\n"]},{"output_type":"execute_result","data":{"text/plain":["38"]},"metadata":{},"execution_count":5}],"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","#example_sent = \"Paragraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. One of the most important of these is a topic sentence.\"\n","example_sent =\"We are attacking on their left flank but are losing many men. we cannot see the enemy army. Nothing else to report. We are ready to attack but are waiting for your orders.\"\n","#example_sent =\"Here are some very simple basic sentences. They won't be very interesting, I'm afraid.\", The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"\n","\n","sen_tokens = sent_tokenize(example_sent)\n","print(sen_tokens)\n","word_tokens = word_tokenize(example_sent)\n","print(word_tokens)\n","len(word_tokens)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMGKUJcGpxkG","executionInfo":{"status":"ok","timestamp":1731910820477,"user_tz":-480,"elapsed":346,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"4652e397-f160-49fe-c623-2a5c2350ec7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'wo', \"n't\", 'be', 'very', 'interesting', ',', 'I', \"'m\", 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', 'cleaning', 'works_', 'on', '*', 'very', 'simple', '*', 'data', '.']]\n"]}],"source":["raw_docs = [\"Here are some very simple basic sentences.\",\n","\"They won't be very interesting, I'm afraid.\",\n","\"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]\n","\n","tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n","print(tokenized_docs)"]},{"cell_type":"markdown","metadata":{"id":"E5R5mBWMpxkG"},"source":["## Noise Removal\n","\n","Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n","\n","This step deals with removal of all types of noisy entities present in the text.\n","\n","A general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary."]},{"cell_type":"markdown","metadata":{"id":"mGgm-WXapxkH"},"source":["### Language Stop Words\n","Stop words usually refers to the most common words in a language. For instance, the English language contains stop words like ‘a’, ‘an’, ‘are’, ‘as’, ‘at’, ‘be’, ‘by’, ‘for’, ‘from’, ‘has’, ‘he’, ‘is’, ‘in’, ‘it’, ‘its’, ‘of’, ‘on’, ‘that’, ‘the’, ‘to’, ‘was’, ‘were’, ‘will’, ‘with’, etc.\n","\n","These words do not carry important meaning and are usually removed from texts. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words.\n","\n","Now, we will learn how to remove stop words with the nltk module."]},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":true,"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"zUIl3FyApxkH","executionInfo":{"status":"ok","timestamp":1731910952524,"user_tz":-480,"elapsed":352,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"43efdc74-56f4-4756-ba95-c3e1c7f3d6a2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'a',\n"," 'about',\n"," 'above',\n"," 'after',\n"," 'again',\n"," 'against',\n"," 'ain',\n"," 'all',\n"," 'am',\n"," 'an',\n"," 'and',\n"," 'any',\n"," 'are',\n"," 'aren',\n"," \"aren't\",\n"," 'as',\n"," 'at',\n"," 'be',\n"," 'because',\n"," 'been',\n"," 'before',\n"," 'being',\n"," 'below',\n"," 'between',\n"," 'both',\n"," 'but',\n"," 'by',\n"," 'can',\n"," 'couldn',\n"," \"couldn't\",\n"," 'd',\n"," 'did',\n"," 'didn',\n"," \"didn't\",\n"," 'do',\n"," 'does',\n"," 'doesn',\n"," \"doesn't\",\n"," 'doing',\n"," 'don',\n"," \"don't\",\n"," 'down',\n"," 'during',\n"," 'each',\n"," 'few',\n"," 'for',\n"," 'from',\n"," 'further',\n"," 'had',\n"," 'hadn',\n"," \"hadn't\",\n"," 'has',\n"," 'hasn',\n"," \"hasn't\",\n"," 'have',\n"," 'haven',\n"," \"haven't\",\n"," 'having',\n"," 'he',\n"," 'her',\n"," 'here',\n"," 'hers',\n"," 'herself',\n"," 'him',\n"," 'himself',\n"," 'his',\n"," 'how',\n"," 'i',\n"," 'if',\n"," 'in',\n"," 'into',\n"," 'is',\n"," 'isn',\n"," \"isn't\",\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'just',\n"," 'll',\n"," 'm',\n"," 'ma',\n"," 'me',\n"," 'mightn',\n"," \"mightn't\",\n"," 'more',\n"," 'most',\n"," 'mustn',\n"," \"mustn't\",\n"," 'my',\n"," 'myself',\n"," 'needn',\n"," \"needn't\",\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'now',\n"," 'o',\n"," 'of',\n"," 'off',\n"," 'on',\n"," 'once',\n"," 'only',\n"," 'or',\n"," 'other',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'out',\n"," 'over',\n"," 'own',\n"," 're',\n"," 's',\n"," 'same',\n"," 'shan',\n"," \"shan't\",\n"," 'she',\n"," \"she's\",\n"," 'should',\n"," \"should've\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'so',\n"," 'some',\n"," 'such',\n"," 't',\n"," 'than',\n"," 'that',\n"," \"that'll\",\n"," 'the',\n"," 'their',\n"," 'theirs',\n"," 'them',\n"," 'themselves',\n"," 'then',\n"," 'there',\n"," 'these',\n"," 'they',\n"," 'this',\n"," 'those',\n"," 'through',\n"," 'to',\n"," 'too',\n"," 'under',\n"," 'until',\n"," 'up',\n"," 've',\n"," 'very',\n"," 'was',\n"," 'wasn',\n"," \"wasn't\",\n"," 'we',\n"," 'were',\n"," 'weren',\n"," \"weren't\",\n"," 'what',\n"," 'when',\n"," 'where',\n"," 'which',\n"," 'while',\n"," 'who',\n"," 'whom',\n"," 'why',\n"," 'will',\n"," 'with',\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\",\n"," 'y',\n"," 'you',\n"," \"you'd\",\n"," \"you'll\",\n"," \"you're\",\n"," \"you've\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves'}"]},"metadata":{},"execution_count":7}],"source":["from nltk.corpus import stopwords\n","\n","stop_words = set(stopwords.words('english'))\n","stop_words"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-145VjmpxkH","executionInfo":{"status":"ok","timestamp":1731910966090,"user_tz":-480,"elapsed":325,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"f2a425b5-3209-4c98-f958-953bc7af70b6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['NLTK',\n"," 'leading',\n"," 'platform',\n"," 'building',\n"," 'Python',\n"," 'programs',\n"," 'work',\n"," 'human',\n"," 'language',\n"," 'data',\n"," '.']"]},"metadata":{},"execution_count":8}],"source":["text = 'NLTK is a leading platform for building Python programs to work with human language data.'\n","text_tokens = word_tokenize(text)\n","new_words = [w for w in text_tokens if not w in stop_words]\n","new_words"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"_H0-4auupxkH","executionInfo":{"status":"ok","timestamp":1731911018098,"user_tz":-480,"elapsed":323,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"d47eee89-4874-47a9-c632-81c967052c95"},"outputs":[{"output_type":"stream","name":"stdout","text":["['We', 'attacking', 'left', 'flank', 'losing', 'many', 'men', '.', 'see', 'enemy', 'army', '.', 'Nothing', 'else', 'report', '.', 'We', 'ready', 'attack', 'waiting', 'orders', '.']\n"]}],"source":["# remove the stopwprds from the word's tocenized text\n","words_filtered = [w for w in word_tokens if not w in stop_words]\n","print(words_filtered)"]},{"cell_type":"markdown","metadata":{"id":"oMqOStyCpxkH"},"source":["### Filter Out Punctuation\n","We can filter out all words that we are not interested in, such as all punctuation. There’s punctuation like commas, apostrophes, quotes, question marks, and more. `string.punctuation` provides a great list of punctuation characters.\n","\n","import string\n","print(string.punctuation)\n","\n","Python has the function `isalpha()` that can be used. The isalpha() method returns True if all the characters are alphabet letters (a-z).\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e1zPgAhapxkI","executionInfo":{"status":"ok","timestamp":1731911038392,"user_tz":-480,"elapsed":320,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"b32a5ced-b611-44c2-c713-46d394db8342"},"outputs":[{"output_type":"stream","name":"stdout","text":["['This', '&', 'is', '[', 'an', ']', 'example', '?', '{', 'of', '}', 'string', '.', 'with', '?', '.', 'punctuation', '!', '!', '!', '!']\n"]},{"output_type":"execute_result","data":{"text/plain":["['This', 'is', 'an', 'example', 'of', 'string', 'with', 'punctuation']"]},"metadata":{},"execution_count":10}],"source":["# remove all punctuation\n","text = 'This &is [an] example? {of} string. with?. punctuation!!!!'\n","\n","text_tokens = word_tokenize(text)\n","print(text_tokens)\n","[w for w in text_tokens if w.isalpha()]\n","# The method isalpha() checks whether the string consists of alphabetic characters only."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emcOXqObpxkI","executionInfo":{"status":"ok","timestamp":1731911077179,"user_tz":-480,"elapsed":341,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"7c088dd3-7abe-4c3b-eed4-32ef5aa3427c"},"outputs":[{"output_type":"stream","name":"stdout","text":["['we', 'attacking', 'left', 'flank', 'losing', 'many', 'men', 'see', 'enemy', 'army', 'nothing', 'else', 'report', 'we', 'ready', 'attack', 'waiting', 'orders']\n"]},{"output_type":"execute_result","data":{"text/plain":["18"]},"metadata":{},"execution_count":11}],"source":["# remove all punctuation\n","words_filtered2 = [w.lower() for w in words_filtered if w.isalpha()] # we use w.lower() to convert text to lowercase\n","print(words_filtered2)\n","len(words_filtered2)"]},{"cell_type":"markdown","metadata":{"id":"G1XRTIvMpxkI"},"source":["### URLs or links\n","In preprocessing process we just need the meaningful words in each text. So, we prefer to remove URLs and links from text. For removing URLs and links, we need to use regular expression."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"EHRIfzZ8pxkI"},"source":["### Regular Expression\n","\n","Regular expression is a sequence of character(s) mainly used to find and replace patterns in a string or file. They are supported by most of the programming languages like python, perl, R, Java and many others.\n","\n","Regular expressions (<a href=\"https://docs.python.org/3/library/re.html\" target=\"_blank\" rel=\"noopener nofollow\">Regular expressions in Python 3</a>) use two types of characters:\n","\n","- <font color=green><b>a) Meta characters</b></font>: these characters have a special meaning. Here’s a complete list of them:\n","<font color=red><b>. ^ $ * + ? { } [ ] | ( ) \\ </b></font>\n","    - <b>Character matches:</b>\n","    \n","        <font color=red><b>.</b></font> :       Matches with any single character except newline\n","        \n","        <font color=red><b>^</b></font> :       Match the start of the string\n","        \n","       <font color=red><b>$</b></font> :       Match the end of the string\n","      \n","       <font color=red><b>[ ]</b></font> :    Matches any single character in a square bracket\n","      \n","      <b>[a-z]</b> :    Matches one of the range of character a,b,...,z\n","      \n","      <b>[^abc]</b> : Matches a character that is not in a,b or c\n","      \n","      <b>a<font color=red>|</font>b</b> :     Matches either a or b, where a and b are string\n","      \n","      <font color=red><b>( )</b></font> :     Groups regular expressions and returns matched text\n","      \n","      <font color=red><b>\\\\</b></font> :       It is used for special meaning characters\n","    <br><br>\n","    - <b> Characters symbols</b>\n","    \n","        <font color=red><b> \\b </b></font>: Matches word boundary\n","        \n","        <font color=red><b> \\d </b></font>: Any digit, equivalent to [0-9]\n","        \n","        <font color=red><b> \\D </b></font>: Any non-digit, equivalent to [^ 0-9]\n","        \n","        <font color=red><b> \\s </b></font>: Any whitespace, equivalent to [\\t\\n\\r\\f\\v]\n","        \n","        <font color=red><b> \\S </b></font>: Any non-whitespace, equivalent to [^ \\t\\n\\r\\f\\v]\n","        \n","        <font color=red><b> \\w </b></font>: Alphanumeric character, equivalent to [a-zA-z0-9_ ]\n","        \n","        <font color=red><b> \\W </b></font>: Non-alphanumeric character, equivalent to [^ a-zA-z0-9_ ]\n","<br><br>\n","    - <b> Repetitions:</b>\n","\n","        <font color=red><b>*</b></font> :       0 or more occurrences\n","        \n","        <font color=red><b>+</b></font> :       1 or more occurrences\n","          \n","        <font color=red><b>?</b></font> :       0 or 1 occurrence\n","        \n","        <font color=red><b>{</font><b>n<font color=red>}</b></font> :       Exactly n repetitions, n>=0\n","        \n","        <font color=red><b>{</font><b>n, <font color=red>}</b></font> :       At least n repetitions\n","        \n","        <font color=red><b>{</font><b> ,n<font color=red>}</b></font> :       At most n repetitions\n","    \n","\n","- <font color=green><b>b) Literals </b></font>(like a,b,1,2…)\n","\n","In Python, we have module “re” that helps with regular expressions. So you need to import library re before you can use regular expressions in Python."]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":true,"id":"JtAHKVHKpxkJ","executionInfo":{"status":"ok","timestamp":1731912089718,"user_tz":-480,"elapsed":318,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}}},"outputs":[],"source":["import re"]},{"cell_type":"markdown","metadata":{"id":"_dSTQ1Y1pxkJ"},"source":["### The most common uses of regular expressions are:\n","- <font color=blue>Search a string </font>(search and match)\n","- <font color=blue>Finding a string </font>(findall)\n","- <font color=blue>Break string into a sub strings</font> (split)\n","- <font color=blue>Replace part of a string </font>(sub)\n","\n","The most commonly used methods which The ‘re’ package provides to perform queries on an input string:\n","\n","- <font color=green><b>re.match(pattern, string)</b></font>:\n","This method finds match if it occurs at start of the string.\n","\n","- <font color=green><b>re.search(pattern, string)</b></font>:\n","It is similar to match() but it doesn’t restrict us to find matches at the beginning of the string only.\n","search() method is able to find a pattern from any position of the string but it only returns the first occurrence of the search pattern.\n","\n","- <font color=green><b>re.findall (pattern, string)</b></font>:\n","This method helps to get a list of all matching patterns. It has no constraints of searching from start or end.\n","\n","- <font color=green><b>re.split(pattern, string, maxsplit=0)</b></font>:\n","This methods helps to split string by the occurrences of given pattern.\n","\n","- <font color=green><b>re.sub(pattern, replace, string)</b></font>:\n","It helps to search a pattern and replace with a new sub string. If the pattern is not found, string is returned unchanged."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IZDn8I-_pxkK","executionInfo":{"status":"ok","timestamp":1731912117199,"user_tz":-480,"elapsed":341,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"34e9629a-3a62-4df0-a098-155b22819b22"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<re.Match object; span=(0, 2), match='AV'>"]},"metadata":{},"execution_count":13}],"source":["#match()\n","result = re.match(r'AV', 'AV Analytics Vidhya AV')\n","result"]},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"elc6JNYzpxkK","executionInfo":{"status":"ok","timestamp":1731912187142,"user_tz":-480,"elapsed":316,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"77bcb9df-031c-4f11-d3fd-aa8a84b3e1c4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<re.Match object; span=(0, 1), match='v'>"]},"metadata":{},"execution_count":14}],"source":["#match()\n","value=\"vrheesville\"\n","m = re.match(r\"vo?\", value)\n","m"]},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"jJsdAwpKpxkK","executionInfo":{"status":"ok","timestamp":1731912202944,"user_tz":-480,"elapsed":345,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"0813e786-2af1-4de2-fe20-0cb18690aa94"},"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}],"source":["#match()\n","value=\"vrheesville\"\n","m = re.match(r\"vo+\", value)\n","print(m)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhRznENfpxkL","executionInfo":{"status":"ok","timestamp":1731912208819,"user_tz":-480,"elapsed":336,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"cf8a3ef2-6637-4408-c4db-c1193579e889"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<re.Match object; span=(0, 2), match='AV'>"]},"metadata":{},"execution_count":17}],"source":["#search()\n","result = re.search(r'AV', 'AV Analytics Vidhya AV')\n","result"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iCMoVq3xpxkL","executionInfo":{"status":"ok","timestamp":1731912211766,"user_tz":-480,"elapsed":414,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"fdbbbba5-9753-4cb8-b72e-363daa32c123"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['AV', 'AV']"]},"metadata":{},"execution_count":18}],"source":["#findall()\n","result = re.findall(r'AV', 'AV Analytics Vidhya AV')\n","result"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-05w_484pxkL","executionInfo":{"status":"ok","timestamp":1731912217384,"user_tz":-480,"elapsed":318,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"67b7baa0-09aa-4d36-901d-d4ca6687971c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Analytics Vidhya AV']"]},"metadata":{},"execution_count":19}],"source":["#findall()\n","result = re.findall(r'(An.*)', 'AV Analytics Vidhya AV')\n","result"]},{"cell_type":"markdown","metadata":{"id":"J8bffXuKpxkL"},"source":["## <font color=green> Exercise A</font>\n","\n","- a) Find all words which start with 'd' or 'p' in the following text.\n","- b) Find all words which contain 'd' or 'p' letter in the following text.\n","\n","   text = \"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\""]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZlHe2uhPpxkL","executionInfo":{"status":"ok","timestamp":1731912972766,"user_tz":-480,"elapsed":325,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"be53dd8d-4c4d-4e61-ceb2-b84ad0da1c82"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['part', 'distribution']"]},"metadata":{},"execution_count":20}],"source":["# a)\n","text=\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\"\n","# Your code here\n","result = re.findall(r'\\b[dp]\\w+', text)\n","result"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U5EeWybIpxkL","executionInfo":{"status":"ok","timestamp":1731913068636,"user_tz":-480,"elapsed":313,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"9bfb823c-d8e3-41ce-8771-611fa9a8221d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['part',\n"," 'address',\n"," 'intermediate',\n"," 'senders',\n"," 'and',\n"," 'intermediate',\n"," 'multiple',\n"," 'addresses',\n"," 'configured',\n"," 'administrator',\n"," 'address',\n"," 'distribution']"]},"metadata":{},"execution_count":22}],"source":["# b)\n","text=\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\"\n","# Your code here\n","result = re.findall(r'\\w*[dp]\\w*', text)\n","result"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ro_mL4XKpxkM","executionInfo":{"status":"ok","timestamp":1731913086410,"user_tz":-480,"elapsed":320,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"9c4febf8-7d38-496d-a435-00afa1968278"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Analyt', 'cs']"]},"metadata":{},"execution_count":23}],"source":["#split()\n","result=re.split(r'i','Analytics')\n","result"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4h9WfLRfpxkM","executionInfo":{"status":"ok","timestamp":1731913088713,"user_tz":-480,"elapsed":337,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"68e85758-6a42-4299-b56a-99cba69cc6d3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['AV An', 'lytics Vidhy', ' AV']"]},"metadata":{},"execution_count":24}],"source":["#split()\n","result=re.split(r'a','AV Analytics Vidhya AV')\n","result"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cGNVDwfKpxkM","executionInfo":{"status":"ok","timestamp":1731913101521,"user_tz":-480,"elapsed":320,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"f407b206-60fa-471e-8584-5f4e7a1b6fdc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['AV An', 'lytics Vidhya AV']"]},"metadata":{},"execution_count":25}],"source":["#split()\n","result=re.split(r'a','AV Analytics Vidhya AV',maxsplit=1)\n","result"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8MmoLUG-pxkM","executionInfo":{"status":"ok","timestamp":1731913308758,"user_tz":-480,"elapsed":325,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"0e4a8504-93cd-44bb-b646-6d1143ae3a50"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['', '1', '2', '3']"]},"metadata":{},"execution_count":27}],"source":["#split()\n","# Separate on one or more non-digit characters in following text.\n","\n","value = \"one 1 two 2 three 3\"\n","result = re.split('\\D+', value)\n","result"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGi2Z-Z3pxkM","executionInfo":{"status":"ok","timestamp":1731913362782,"user_tz":-480,"elapsed":339,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"48f4d02e-9925-4eb6-cf6c-70a73e00f049"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Ethics', 'Ethical']"]},"metadata":{},"execution_count":28}],"source":["#split()\n","# finding all words which include 'E' in bellow tweet\n","\n","tweet2 = '@UN @UN_Women \"Ethics are built right into the ideals and objectives of the United Nations\" \\\n","#UNSG @ NY Society for Ethical Culture bit.ly/2guVelr'\n","\n","result = re.findall(r'w*[E]\\w*', tweet2)\n","result"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UJTmwT-pxkM","executionInfo":{"status":"ok","timestamp":1731913413328,"user_tz":-480,"elapsed":323,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"12ec91d4-5c88-4b3b-a18b-7887a481b747"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\"Ethics', 'Ethical']"]},"metadata":{},"execution_count":29}],"source":["text1=re.split(' ',tweet2)\n","text2=[w for w in text1 if re.findall(r'\\w*[E]\\w*', w)]\n","text2"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cuKcg77mpxkM","executionInfo":{"status":"ok","timestamp":1731913417482,"user_tz":-480,"elapsed":325,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"67e777b2-ff59-4214-c220-d824e9f3d62a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Ethics', 'Ethical']"]},"metadata":{},"execution_count":30}],"source":["text1 = word_tokenize(tweet2)\n","text2=[w for w in text1 if re.findall(r'\\w*[E]\\w*', w)]\n","text2"]},{"cell_type":"code","execution_count":31,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"fwAXsHympxkT","executionInfo":{"status":"ok","timestamp":1731913439394,"user_tz":-480,"elapsed":406,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"f8a4e855-23d2-40c0-b57f-4886f6259739"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'AV is largest Analytics community of the World'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}],"source":["#sub()\n","result=re.sub(r'India','the World','AV is largest Analytics community of India')\n","result"]},{"cell_type":"markdown","metadata":{"id":"vOapI5ECpxkT"},"source":["#### Two useful methods\n","\n","- <font color=green><b>join()</b></font>: The method returns a string in which the string elements of sequence have been joined by str separator.\n","\n","- <font color=green><b>strip()</b></font>: The method returns a copy of the string in which all characters have been stripped from the beginning and the end of the string (default whitespace characters)."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6At_gR1BpxkT","executionInfo":{"status":"ok","timestamp":1731913896313,"user_tz":-480,"elapsed":340,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"e5e8d911-3c95-4c0e-907a-feb2d3d06d16"},"outputs":[{"output_type":"stream","name":"stdout","text":["['AV', 'is', 'largest', 'Analytics', 'community', 'of', 'India']\n"]}],"source":["# join()\n","tweet2 = 'AV is largest Analytics community of India'\n","text1 = tweet2.split(' ')\n","print(text1)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"u-P7pZ98pxkT","executionInfo":{"status":"ok","timestamp":1731913903281,"user_tz":-480,"elapsed":326,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"c309476d-3baf-47b6-a36a-52cb389cdfca"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'AV is largest Analytics community of India'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}],"source":["text2 = ' '.join(text1)\n","text2"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P32qDnLNpxkT","executionInfo":{"status":"ok","timestamp":1731913936529,"user_tz":-480,"elapsed":328,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"854d8fe3-3fda-4444-dd19-25136d16ad49"},"outputs":[{"output_type":"stream","name":"stdout","text":["quick brown  fox  jumped over the lazy dog.  \n","\n"]}],"source":["# strip()\n","text8 ='    quick brown  fox  jumped over the lazy dog.  \\n '\n","text9 = text8.strip(' ')\n","print(text9)"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"53MEWTkgpxkU","executionInfo":{"status":"ok","timestamp":1731913940523,"user_tz":-480,"elapsed":325,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"554cb00c-a234-4657-fff7-c1fa1b54679e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'This is a tweet with a url: and there is no any url'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}],"source":["# remove URLs or links\n","\n","tweet = 'This is a tweet with a url:http://t.co/0DlGChTBIx and there is no any url'\n","tweet_clean = re.sub(r\"http\\S+\", \"\", tweet)\n","tweet_clean"]},{"cell_type":"code","execution_count":36,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"kZ16QCWNpxkU","executionInfo":{"status":"ok","timestamp":1731913964290,"user_tz":-480,"elapsed":320,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"89e5f240-ee78-43ac-bf97-fcc38fb40fe2"},"outputs":[{"output_type":"stream","name":"stdout","text":["START: 123\n","END: 123\n","\n","\n","START: 4cat\n","\n","\n","END: dog5\n","\n","\n","START: 6mouse\n","\n","\n","\n","\n"]}],"source":["# what do this code?\n","list = [\"123\", \"4cat\", \"dog5\", \"6mouse\", \"mouse\"]\n","\n","for w in list:\n","    m = re.match(\"^\\d\", w)\n","    if m:\n","        print(\"START:\", w)\n","\n","    m = re.match(\".*\\d$\", w)\n","    if m:\n","        print(\"END:\", w)\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"xbZ8aUk1pxkU"},"source":["## <font color=green> Exercise B</font>\n","Write a piece of code to extract:      \n","1) the words which start with 'vi' without using ^ from the text bellow.\n","\n","    text = ' visit123 \"Ethics are21 view right into21 the via ideals and objectives of the United Nations\" \\ #UNSG @21 NY23 Society for134  a14 Ethical43 view23 vital'\n","     \n","2) all words exept those including 'dog' from given text.\n","\n","       text = '100cat 223cat 534dog 400cat 500car 345dog 847bar'\n","\n","3) all words exept numbers from the text bellow.\n","\n","       text = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n","\n","4) Email address from following text.\n","\n","    text='John.Smith@example.com Ethics are built right into the ideals and objectives of the United Nations\" \\ #UNSG @21 NY Society for Ethical Culture local-part@domain.org'\n"]},{"cell_type":"code","execution_count":43,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"dOtorfECpxkV","executionInfo":{"status":"ok","timestamp":1731917273266,"user_tz":-480,"elapsed":332,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"539595e9-5cc2-4a0d-de68-034954b60995"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\xa0visit123\\xa0\"Ethics\\xa0are21\\xa0view\\xa0right\\xa0into21\\xa0the\\xa0via\\xa0ideals\\xa0and\\xa0objectives\\xa0of\\xa0the\\xa0United\\xa0Nations\"\\xa0\\\\\\xa0#UNSG\\xa0@21\\xa0NY23\\xa0Society\\xa0for134\\xa0\\xa0a14\\xa0Ethical43\\xa0view23\\xa0vital']"]},"metadata":{},"execution_count":43}],"source":["# 1)\n","# Your code here\n","text = ' visit123 \"Ethics are21 view right into21 the via ideals and objectives of the United Nations\" \\ #UNSG @21 NY23 Society for134  a14 Ethical43 view23 vital'\n","text1 = text.split(' ')\n","text2 = [w for w in text1 if re.findall('(vi*)',w)]\n","text2"]},{"cell_type":"code","execution_count":47,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"ToM65O5JpxkV","executionInfo":{"status":"ok","timestamp":1731917449843,"user_tz":-480,"elapsed":347,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"ce9f52f5-69af-4538-ef87-e4f1233f43c7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['100cat\\xa0223cat\\xa0534dog\\xa0400cat\\xa0500car\\xa0345dog\\xa0847bar']"]},"metadata":{},"execution_count":47}],"source":["# 2)\n","# Your code here\n","text = '100cat 223cat 534dog 400cat 500car 345dog 847bar'\n","text1 = text.split(' ')\n","text2 = [w for w in text1 if re.findall('(\\d{3})[^dog]',w)]\n","text2"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"7M1Anpm7pxkV"},"outputs":[],"source":["# 3)\n","# Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"wv0fe_grpxkV"},"outputs":[],"source":["# 4)\n","# Your code here"]},{"cell_type":"markdown","metadata":{"id":"VF_jMUYxpxkV"},"source":["### Social media entities\n","Over the years, there has been a significant increase in the amount of short text available, as social networks like LinkedIn, Facebook, Twitter and the like, have risen in popularity. Such data stores often contain information that is relevant and potentially valuable. In analysing social medis posts, we will face with a lot of noise in the data, particularly URLs, the hashtags, twitter Ids, and @user. We also use regular expression to remove this type of noises from the text.\n"]},{"cell_type":"markdown","metadata":{"id":"tzLB3u-fpxkV"},"source":["### <font color=green> Exercise C</font>\n","\n","Write a piece of code to extract:\n","- 1) hashtags from following tweet.\n","\n","    - a) tweet1 = \"@nltk Text analysis is awesome! #regex #pandas #python\"\n","    - b) tweet2 = [\"Here are some very #simple basic #sentences.\",\n","\"They won't be #very interesting, I'm #afraid.\",\n","\"The #point of these #examples is to _learn how basic text #cleaning works_ on *very simple* data.\"]\n","\n","   \n","    \n","- 2) callouts from given tweet.\n","\n","    tweet2 = '@UN @UN_Women \"Ethics are built right into the ideals and objectives of the United Nations\" \\ #UNSG @ NY Society for Ethical Culture bit.ly/2guVelr'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhdU1QWppxkV"},"outputs":[],"source":["# 1a)\n","# Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdGoKGiZpxkV"},"outputs":[],"source":["# 1b)\n","# Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94flgHTFpxkV"},"outputs":[],"source":["# 2)\n","# Your code here"]},{"cell_type":"markdown","metadata":{"id":"dR2C1moKpxkV"},"source":["# Lexicon Normalization\n","\n","Another type of textual noise is about the multiple representations exhibited by single word.\n","\n","For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma).\n","\n","The most common lexicon normalization practices are :\n","\n","- <b> Stemming</b>:<br>\n","Stemming is a process of reducing words to their word stem, base or root form (For example “fishing”, “fished” and “fisher” all reduce to the stem “fish”). The main algorithm is Porter stemming algorithm which removes common morphological and inflexional endings(“ing”, “ly”, “es”, “s” etc) from words.\n","    \n","    \n","- <b>Lemmatization</b>: <br>\n","The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. **As opposed to stemming, lemmatization does not simply chop off inflections**. Instead it uses lexical knowledge bases to get the correct base forms of words.\n","    - it makes use of:\n","        - vocabulary (dictionary importance of words)\n","        - morphological analysis (word structure and grammar relations)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quJs8BJWpxkW","executionInfo":{"status":"ok","timestamp":1731917595468,"user_tz":-480,"elapsed":329,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"cf427448-1b80-4c5a-f78a-0f89116d837a"},"outputs":[{"output_type":"stream","name":"stdout","text":["the\n","littl\n","yellow\n","dog\n","bark\n","at\n","the\n","cat\n",".\n","think\n",",\n","play\n",",\n","cat\n",",\n","leav\n"]}],"source":["#Stemming\n","from nltk.stem import PorterStemmer\n","\n","new_text=\"the little yellow dog barked at the cat. thinking, plays, cats, leaves\"\n","# new_text = \"There are several types of stemming algorithms.\"\n","\n","words = word_tokenize(new_text)\n","\n","ps = PorterStemmer()\n","for w in words:\n","    print(ps.stem(w))"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hpaDLFCpxkW","executionInfo":{"status":"ok","timestamp":1731917598795,"user_tz":-480,"elapsed":338,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"c7fec0aa-726f-4a03-ed07-2da5018475d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["python\n","python\n","python\n","python\n","pythonli\n","eventuel\n","darf\n"]}],"source":["#Stemming\n","example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\", 'eventuellement', 'darfed']\n","for w in example_words:\n","    print(ps.stem(w))"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NxxSjzw4pxkW","executionInfo":{"status":"ok","timestamp":1731917609364,"user_tz":-480,"elapsed":4824,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"3e65c912-98d3-471c-8de8-9843a7a06981"},"outputs":[{"output_type":"stream","name":"stdout","text":["cat\n","goose\n","rock\n","life\n","leaf\n","worse\n","runing\n"]}],"source":["#Lemmatization\n","from nltk.stem import WordNetLemmatizer\n","\n","lemmatizer = WordNetLemmatizer()\n","words =(\"cats\",\"geese\",\"rocks\",\"lives\",\"leaves\",\"worse\",\"runing\")\n","# words = [\"been\", \"had\", \"done\", \"languages\", \"cities\", \"mice\"]\n","\n","for w in words:\n","    print(lemmatizer.lemmatize(w))"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ApdrMFpGpxkW","executionInfo":{"status":"ok","timestamp":1731917617807,"user_tz":-480,"elapsed":318,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"ce212c5d-a0c4-4f41-acf3-18b7d3551e59"},"outputs":[{"output_type":"stream","name":"stdout","text":["good\n","best\n","bad\n","runing\n","run\n"]}],"source":["print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n","print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n","print(lemmatizer.lemmatize(\"worse\", pos=\"a\"))\n","print(lemmatizer.lemmatize(\"runing\"))\n","print(lemmatizer.lemmatize(\"runing\",'v'))"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"JPV6HrGKpxkW"},"source":["# Part of speech tagging\n","\n","Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. Recall from your high school grammer that part of speech are these verb classes like nouns, verbs, adjectives, adverbs etc.\n","\n","Here is a <b>list of pos-tags</b>, what they mean, and some examples:(<font color=blue>Tag</font>, Word class, <font color=green>Example</font>)\n","\n","<font color=blue>CC</font>\tcoordinating conjunction <br>\n","<font color=blue>CD</font>\tcardinal digit<br>\n","<font color=blue>DT</font>\tdeterminer<br>\n","<font color=blue>EX\t</font>existential there (like: <font color=green>\"there is\"</font> ... think of it like <font color=green>\"there exists\"</font>)<br>\n","<font color=blue>FW\t</font>foreign word<br>\n","<font color=blue>IN\t</font>preposition/subordinating conjunction<br>\n","<font color=blue>JJ\t</font>adjective\t<font color=green>'big'</font><br>\n","<font color=blue>JJR\t</font>adjective, comparative\t<font color=green>'bigger'</font><br>\n","<font color=blue>JJS\t</font>adjective, superlative\t<font color=green>'biggest'</font><br>\n","<font color=blue>LS\t</font>list marker\t<font color=green>1)</font><br>\n","<font color=blue>MD\t</font>modal\t<font color=green>could, will</font><br>\n","<font color=blue>NN\t</font>noun, singular <font color=green>'desk'</font><br>\n","<font color=blue>NNS\t</font>noun plural\t<font color=green>'desks'</font><br>\n","<font color=blue>NNP\t</font>proper noun, singular\t<font color=green>'Harrison'</font><br>\n","<font color=blue>NNPS\t</font>proper noun, plural\t<font color=green>'Americans'</font><br>\n","<font color=blue>PDT\t</font>predeterminer\t<font color=green>'all the kids'</font><br>\n","<font color=blue>POS\t</font>possessive ending\tparent<font color=green>'s</font><br>\n","<font color=blue>PRP\t</font>personal pronoun\t<font color=green>I, he, she</font><br>\n","<font color=blue>PRPS\t</font>possessive pronoun\t<font color=green>my, his, hers</font><br>\n","<font color=blue> RB\t</font>adverb\t<font color=green>very, silently</font><br>\n","<font color=blue> RBR</font>\tadverb, comparative\t<font color=green>better</font><br>\n","<font color=blue>RBS</font>\tadverb, superlative\t<font color=green>best</font><br>\n","<font color=blue>RP\t</font>particle<font color=green>\tgive up</font><br>\n","<font color=blue>TO\t</font>to\tgo <font color=green>'to' </font>the store.<br>\n","<font color=blue>UH\t</font>interjection\t<font color=green>errrrrrrrm</font><br>\n","<font color=blue>VB\t</font>verb, base form\t<font color=green>take</font><br>\n","<font color=blue>VBD\t</font>verb, past tense\t<font color=green>took</font><br>\n","<font color=blue>VBG</font>\tverb, gerund/present participle\t<font color=green>taking</font><br>\n","<font color=blue>VBN\t</font>verb, past participle\t<font color=green>taken</font><br>\n","<font color=blue>VBP\t</font>verb, sing. present, non-3d\t<font color=green>take</font><br>\n","<font color=blue>VBZ\t</font>verb, 3rd person sing. present\t<font color=green>takes</font><br>\n","<font color=blue>WDT\t</font>wh-determiner\t<font color=green>which</font><br>\n","<font color=blue>WP\t</font>wh-pronoun\t<font color=green>who, what</font><br>\n","<font color=blue>WPS</font>possessive wh-pronoun\t<font color=green>whose</font><br>\n","<font color=blue> WRB\t</font>wh-abverb\t<font color=green>where, when"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dzxHVpTUpxkW","executionInfo":{"status":"ok","timestamp":1731917626629,"user_tz":-480,"elapsed":308,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"3e10bb1a-73de-429f-8874-de231a440a6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN')]\n"]}],"source":["#Part of speech tagging\n","import nltk\n","sample_text = \"the little yellow dog barked at the cat\"\n","# sample_text = \"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n","\n","words = word_tokenize(sample_text)\n","\n","tagged = nltk.pos_tag(words)\n","print(tagged)"]},{"cell_type":"markdown","metadata":{"id":"ve7TWP8epxkW"},"source":["# Named entity recognition\n","Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.).\n","\n","- The named entities such as:\n","     - Person names - <font color=green> Eddy Bonte, President Obama</font>\n","     - Location names - <font color=green>Murray River, Mount Everest</font>\n","     - Company names -<font color=green> Washington Monument, Stonehenge</font>\n","     - Date - <font color=green>June, 2008-06-29</font>\n","     - Time - <font color=green>two fifty a m, 1:30 p.m.</font>\n","     - Percent - <font color=green>twenty pct, 18.75 %</font>\n","     - Money - <font color=green>175 million Canadian Dollars, GBP 10.40</font>\n","     - GPE (Geographical Entity) - <font color=green>South East Asia, Midlothian</font>\n","     - Organization -<font color=green> Georgia-Pacific Corp., WHO</font>"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFYmCbXepxkW","executionInfo":{"status":"ok","timestamp":1731917632189,"user_tz":-480,"elapsed":651,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"fd260a4a-ed85-4796-faaa-78d3dc4757d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (PERSON Bill/NNP)\n","  works/VBZ\n","  for/IN\n","  Apple/NNP\n","  so/IN\n","  he/PRP\n","  went/VBD\n","  to/TO\n","  (GPE Boston/NNP)\n","  for/IN\n","  a/DT\n","  conference/NN\n","  ./.)\n"]}],"source":["from nltk import word_tokenize, pos_tag, ne_chunk\n","\n","text = \"Bill works for Apple so he went to Boston for a conference.\"\n","\n","pos_tag_words = pos_tag(word_tokenize(text))\n","print(ne_chunk(pos_tag_words))"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXbS4H5TpxkW","executionInfo":{"status":"ok","timestamp":1731917636532,"user_tz":-480,"elapsed":1000,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"76f6eb4e-2edd-4f5a-c7ae-85230beabdf1"},"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (PERSON Sara/NNP)\n","  likes/VBZ\n","  to/TO\n","  visit/VB\n","  different/JJ\n","  parts/NNS\n","  of/IN\n","  malaysia/NN\n","  ./.\n","  She/PRP\n","  visits/VBZ\n","  (PERSON Cameron/NNP)\n","  hilands/NNS\n","  ,/,\n","  (GPE Frozen/NNP)\n","  hills/NNS\n","  ,/,\n","  (GPE Port/NNP)\n","  dickson/NN\n","  and/CC\n","  (PERSON Melaka/NNP)\n","  until/IN\n","  now/RB\n","  ./.)\n"]}],"source":["sample_text=\"Sara likes to visit different parts of malaysia. She visits Cameron hilands, Frozen hills, Port dickson and Melaka until now.\"\n","# sample_text=\" I'm going to Germany this Monday.\"\n","# sample_text=\"Mark and John are working at Google.\"\n","# sample_text=\"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n","# sample_text=\" When, after the 2010 election, Wilkie, Rob, Oakeshott, Tony Windsor and the Greens agreed to support Labor, they gave just two guarantees: confidence and supply.\"\n","words = word_tokenize(sample_text)\n","\n","tagged = nltk.pos_tag(words)\n","namedEnt=nltk.ne_chunk(tagged)\n","print(namedEnt)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"5QmHS4akpxkX"},"source":["# Some useful functions"]},{"cell_type":"markdown","metadata":{"id":"JUcz7l-DpxkX"},"source":["### Finding specific words\n","- long words:words that are more than 3 letters long\n","- Capitalized words:the words that start with a capital letter [a-z]\n","- Words that end / start with a specific letter"]},{"cell_type":"code","execution_count":63,"metadata":{"collapsed":true,"id":"98hE5PHxpxkX","executionInfo":{"status":"ok","timestamp":1731918387611,"user_tz":-480,"elapsed":313,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}}},"outputs":[],"source":["text1 = \"We are attacking on their left flank but are losing many men. We cannot see the enemy army. Nothing else to report. We are ready to attack but are waiting for your orders.\"\n","text2 = word_tokenize(text1)"]},{"cell_type":"code","execution_count":64,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"IGTOcz6UpxkX","executionInfo":{"status":"ok","timestamp":1731918389635,"user_tz":-480,"elapsed":294,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"782274c7-0f26-4645-8198-b9c64eaf5553"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['attacking',\n"," 'their',\n"," 'left',\n"," 'flank',\n"," 'losing',\n"," 'many',\n"," 'enemy',\n"," 'army',\n"," 'Nothing',\n"," 'else',\n"," 'report',\n"," 'ready',\n"," 'attack',\n"," 'waiting',\n"," 'your',\n"," 'orders']"]},"metadata":{},"execution_count":64}],"source":["# Long words\n","text3 = [w for w in text2 if len(w)>3]\n","text3"]},{"cell_type":"code","execution_count":65,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"BfcMpRhppxkX","executionInfo":{"status":"ok","timestamp":1731918391960,"user_tz":-480,"elapsed":317,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"4f6e396e-2736-40b7-c6af-6ce6df37c03f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['We', 'We', 'Nothing', 'We']"]},"metadata":{},"execution_count":65}],"source":["# Capitalized words\n","[w for w in text2 if w.istitle()]"]},{"cell_type":"code","execution_count":66,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"mI_0hrI0pxkX","executionInfo":{"status":"ok","timestamp":1731918393861,"user_tz":-480,"elapsed":323,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"72efd3a9-46c0-40e8-aeb1-aec38f3dcf42"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['We', 'are', 'are', 'We', 'see', 'the', 'else', 'We', 'are', 'are']"]},"metadata":{},"execution_count":66}],"source":["# Words that end with e\n","[w for w in text2 if w.endswith('e')]"]},{"cell_type":"code","execution_count":67,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"bZn116VqpxkX","executionInfo":{"status":"ok","timestamp":1731918395869,"user_tz":-480,"elapsed":344,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"d81f2411-4c2a-4080-cc27-1df5b2d4d063"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['their', 'the', 'to', 'to']"]},"metadata":{},"execution_count":67}],"source":["# Words that start with t\n","[w for w in text2 if w.startswith('t')]"]},{"cell_type":"markdown","metadata":{"id":"xpCG54xTpxkY"},"source":["### Finding unique words: Using set()"]},{"cell_type":"code","execution_count":68,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"YB8QrGK0pxkY","executionInfo":{"status":"ok","timestamp":1731918398050,"user_tz":-480,"elapsed":312,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"3b5ac951-1881-4f4a-d45e-3cccc30dc8da"},"outputs":[{"output_type":"stream","name":"stdout","text":["6\n","{'be', 'or', 'To', 'to', 'not'}\n"]}],"source":["#Finding unique words\n","text3 = \"To be or not to be\"\n","text4 = text3.split(' ')\n","print(len(text4))\n","print(set(text4))"]},{"cell_type":"code","execution_count":69,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"PmsFg98PpxkY","executionInfo":{"status":"ok","timestamp":1731918400258,"user_tz":-480,"elapsed":308,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"b0cf769d-84e5-4c9d-ca56-ecaec96d1a3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'not', 'be', 'to', 'or'}\n"]},{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":69}],"source":["print(set([w.lower() for w in text4]))\n","len(set([w.lower() for w in text4]))"]},{"cell_type":"markdown","metadata":{"id":"MActJq8_pxkY"},"source":["### Frequency of words"]},{"cell_type":"code","execution_count":70,"metadata":{"collapsed":true,"id":"-FxxYddSpxkY","executionInfo":{"status":"ok","timestamp":1731918403031,"user_tz":-480,"elapsed":306,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}}},"outputs":[],"source":["from nltk.probability import FreqDist"]},{"cell_type":"markdown","metadata":{"id":"zulMUAeBpxkY"},"source":["text=\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\""]},{"cell_type":"code","execution_count":71,"metadata":{"collapsed":true,"id":"e4RdPsIxpxkY","executionInfo":{"status":"ok","timestamp":1731918405229,"user_tz":-480,"elapsed":329,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}}},"outputs":[],"source":["text =\"The local part of an email address has no significance for intermediate mail relay systems other than the final mailbox host. Email senders and intermediate relay systems must not assume it to be case-insensitive, since the final mailbox host may or may not treat it as such. A single mailbox may receive mail for multiple email addresses, if configured by the administrator. Conversely, a single email address may be the alias to a distribution list to many mailboxes\"\n","text_tokens = word_tokenize(text)\n"]},{"cell_type":"code","execution_count":72,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"utuu9OJQpxkY","executionInfo":{"status":"ok","timestamp":1731918407554,"user_tz":-480,"elapsed":552,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"58dafd3e-6e72-4333-c6df-d4c3ba070f1e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["FreqDist({'the': 5, 'email': 4, 'may': 4, 'mailbox': 3, '.': 3, 'to': 3, ',': 3, 'a': 3, 'address': 2, 'for': 2, ...})"]},"metadata":{},"execution_count":72}],"source":["text_tokens1 = [w.lower() for w in text_tokens]\n","\n","dist = FreqDist(text_tokens1)\n","len(dist)\n","dist"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"txeoQGf5pxkY","executionInfo":{"status":"ok","timestamp":1731918409693,"user_tz":-480,"elapsed":319,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"591b7ae1-e09e-4206-e2ee-965c63233842"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['the', 'local', 'part', 'of', 'an', 'email', 'address', 'has', 'no', 'significance', 'for', 'intermediate', 'mail', 'relay', 'systems', 'other', 'than', 'final', 'mailbox', 'host', '.', 'senders', 'and', 'must', 'not', 'assume', 'it', 'to', 'be', 'case-insensitive', ',', 'since', 'may', 'or', 'treat', 'as', 'such', 'a', 'single', 'receive', 'multiple', 'addresses', 'if', 'configured', 'by', 'administrator', 'conversely', 'alias', 'distribution', 'list', 'many', 'mailboxes'])"]},"metadata":{},"execution_count":73}],"source":["vocab = dist.keys()\n","vocab"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ByC92U84pxkZ","executionInfo":{"status":"ok","timestamp":1731918412029,"user_tz":-480,"elapsed":327,"user":{"displayName":"raja zareef","userId":"18312074659346131209"}},"outputId":"55441852-41de-4bed-97cf-267a518a0657"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":74}],"source":["dist['the']"]},{"cell_type":"markdown","metadata":{"id":"xcMDhPOOpxkZ"},"source":["### <font color=green> Exercise E</font>\n","\n","- Find words have length at least 3 and occur at least 3 times in previous text."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"VatAuqATpxkZ"},"outputs":[],"source":["# your code is here"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}